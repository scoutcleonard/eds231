---
title: "EDS 231: Assignment 1"
author: "Scout Leonard"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
header-includes:
  - \setlength{\parindent}{1em}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, 
                      fig.height = 5, 
                      echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)
```

# Load Libraries 

```{r load_libraries}
library(here)
library(pdftools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
library(knitr)
```

# Set Up

## Read in data: 

```{r}
#grab data here: 
comments_df <- read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv")
```

## Corpus: 

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")

epa_corp.stats <- summary(epa_corp)

head(epa_corp.stats, n = 10) %>% 
  kable()
```

## Tokenize Corpus: 

```{r tokens}
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)

#I added some project-specific stop words here
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")

toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")
```

## Convert tokens to a document frame matrix:

```{r dfm}
#construct dfm from tokens
dfm_comm <- dfm(toks1, tolower = TRUE)

#apply a stemmer to words in dfm
dfm <- dfm_wordstem(dfm_comm)

#remove terms only appearing in one doc (min_termfreq = 10)
dfm <- dfm_trim(dfm, min_docfreq = 2) 

#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 

#comments_df <- dfm[sel_idx, ]
dfm <- dfm[sel_idx, ]
```

## LDA Modelling: 

### Write the model: 

```{r LDS_modelling}
topicModel_k9 <- LDA(dfm,
                     k = 9,
                     method = "Gibbs",
                     control = list(iter = 500, verbose = 25))
```

### Return results:

```{r}
tmResult <- posterior(topicModel_k9)

beta <- tmResult$terms #get beta from results

terms(topicModel_k9, 10)
```

### Visualize results: 

```{r LDAvis}
#load libraries
library(LDAvis)
library("tsne")

svd_tsne <- function(x) tsne(svd(x)$u)

json <- createJSON(
  phi = tmResult$terms,
  theta = tmResult$topics,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="",
                   ylab="")
)

serVis(json)
```

# Assignment

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis. 

## Model 1: `k = 5`

### Write model:

```{r}
topicModel_k5 <- LDA(dfm,
                     k = 5,
                     method = "Gibbs",
                     control = list(iter = 500, verbose = 25))
```

### Return results:

```{r}
tmResult_k5 <- posterior(topicModel_k5)

beta <- tmResult_k5$terms #get beta from results

terms(topicModel_k5, 10)
```

### Visualize Results:

```{r LDAvis5}
json_k5 <- createJSON(
  phi = tmResult_k5$terms,
  theta = tmResult_k5$topics,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="",
                   ylab="")
)

serVis(json_k5)
```

## Model 2: `k = 20`

### Write model:

```{r}
topicModel_k20 <- LDA(dfm,
                     k = 20,
                     method = "Gibbs",
                     control = list(iter = 500, verbose = 25))
```


### Return results:

```{r}
tmResult_k20 <- posterior(topicModel_k20)

beta <- tmResult_k20$terms #get beta from results

terms(topicModel_k20, 10)
```

### Visualize results:

```{r LDAvis20}
json_k20 <- createJSON(
  phi = tmResult_k20$terms,
  theta = tmResult_k20$topics,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="",
                   ylab="")
)

serVis(json_k20)
```

## Model 3: `k = 3`

### Write model:

```{r}
topicModel_k3 <- LDA(dfm,
                     k = 3,
                     method = "Gibbs",
                     control = list(iter = 500, verbose = 25))
```

### Return results:

```{r}
tmResult_k3 <- posterior(topicModel_k3)

beta <- tmResult_k3$terms #get beta from results

terms(topicModel_k3, 3)
```

### Visualize results:

```{r LDAvis3}
json_k3 <- createJSON(
  phi = tmResult_k3$terms,
  theta = tmResult_k3$topics,
  doc.length = rowSums(dfm),
  vocab = colnames(dfm),
  term.frequency = colSums(dfm),
  mds.method = svd_tsne,
  plot.opts = list(xlab="",
                   ylab="")
)

serVis(json_k3)
```

I think that overall, my model with k = 3 was the best number of topics for any of the models I tried. I think this based on the ratio of overall term freqnecy to estimated term frequency with the selected topic from the model, as output by the `serVis()` function. For the model with `k = 3`,  these seemed to be the closest compared to the models with `k = 5` and `k = 20`. 

Also, there are fewer topics with overlapping dimensions according to the intertopic distance map for the `k = 3` model compared to the other two models. `k = 5` similarly had more intertopic distance, but the term salience was not as strong as described in the previous paragraph. 