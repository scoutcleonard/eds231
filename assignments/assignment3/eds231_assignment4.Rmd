---
title: "EDS 231: Assignment 3"
author: "Scout Leonard"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
header-includes:
  - \setlength{\parindent}{1em}
  - \usepackage{float}
---

# Load Libraries

```{r packages, results='hide', message=FALSE, warning=FALSE}
packages=c("quanteda.sentiment",
           "quanteda.textstats",
           "tidyverse",
           "tidytext",
           "lubridate",
           "here",
           "wordcloud", #visualization of common words in the data set
           "reshape2",
           "quanteda",
           "knitr") #devtools::install_github("quanteda/quanteda.sentiment") #not available currently through CRAN

for (i in packages) {
  if (require(i,character.only=TRUE)==FALSE) {
    install.packages(i,repos='http://cran.us.r-project.org')
  }
  else {
    require(i,character.only=TRUE)
  }
}
```

# Import Data

```{r}
raw_tweets <- read.csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv", header=TRUE)

dat<- raw_tweets[,c(5,7)] # Extract Date and Title fields

tweets <- tibble(text = dat$Title,
                  id = seq(1:length(dat$Title)),
                 date = as.Date(dat$Date,'%m/%d/%y'))
```

\newpage

You will use the tweet data from class today for each part of the following assignment.

# Part 1

Think about how to further clean a twitter data set. Let's assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

## Remove mentions

```{r}
#remove mentions
tweets$text <- str_remove(tweets$text, "@[a-z,A-Z]*")
tweets$text <- str_remove(tweets$text, "[:digit:]")

#tokenise tweets and remove stop words
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word")

#clean tokens
##remove numbers
clean_tokens <- str_remove_all(words$word, "[:digit:]")
##remove mentions
clean_tokens <- str_remove_all(clean_tokens, "@[a-z,A-Z]*")
##removes appostrophes
clean_tokens <- gsub("â€™s", '', clean_tokens)
## remove weird twitter and internet things
clean_tokens <- str_remove_all(clean_tokens, "https")
clean_tokens <- str_remove_all(clean_tokens, "t.co")

words$clean <- clean_tokens

#remove the empty strings
tib <-subset(words, clean != "")

#reassign
words <- tib
```

\newpage

# Part 2

Compare the ten most common terms in the tweets per day.  Do you notice anything interesting?

```{r}
words_freq <- words %>% 
  group_by(clean) %>% 
  summarise(n()) %>% 
  top_n(10) %>% 
  rename("freq" = "n()") %>% 
  select(clean)

top_10_words <- inner_join(words_freq, words, by = "clean") %>% 
  group_by(date, clean) %>% 
  summarize(n()) %>% 
  rename("freq" = "n()")
```

```{r}
ggplot(data = top_10_words, aes(x = date, y = freq)) +
  geom_line(aes(color = clean)) +
  theme_minimal() +
  labs(title = "Top 10 Most Frequent Words in IPCC-related Tweets",
       subtitle = "Tweets Leading Up to the 2022 Report",
       x = "Date",
       y = "Frequency",
       color = "Word") +
  theme(plot.title = element_text(hjust = 0.7, size = 15),
        plot.subtitle = element_text(hjust = 0.5))
```

I notice that there is a spike in all the top 10 words on April 4th, when the report is published. I assume this is because there is lots of coverage on this day compared to others. 

"IPCC", "report", and "climate" are all the most frequent words the day the report is published and tend to stay highest for several days after, even as the frequency of all words declines. 

\newpage

# Part 3

Adjust the wordcloud in the "wordcloud" chunk by coloring the positive and negative words so they are identifiable.

```{r fig.width= 4, fig.height=4}
words %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("darkslateblue", "goldenrod"),
                   max.words = 100)
```

\newpage

# Part 4

Let's say we are interested in the most prominent entities in the Twitter discussion.  Which are the top 10 most tagged accounts in the data set. Hint: the "explore_hashtags" chunk is a good starting point.

```{r}
corpus <- corpus(dat$Title)

tokens <- tokens(corpus)

#clean it up
tokens <- tokens(tokens, remove_punct = TRUE,
                      remove_numbers = TRUE)

tokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda

tokens <- tokens_wordstem(tokens) 

tokens <- tokens_tolower(tokens)

at_tweets <- tokens(corpus, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "@*")

dfm_at<- dfm(at_tweets) #document feature matrix

tstat_freq <- textstat_frequency(dfm_at, n = 100)
head(tstat_freq, 10)

#tidytext gives us tools to convert to tidy from non-tidy formats
at_tib<- tidy(dfm_at)

at_tib %>%
   group_by(term) %>% 
  summarise(n()) %>% 
  top_n(10) %>% 
  knitr::kable()
```

\newpage

# Part 5

The Twitter data download comes with a variable called "Sentiment" that must be calculated by Brandwatch.  Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch's (hint: you'll need to revisit the "raw_tweets" data frame).   

## My method 

```{r}
#give raw tweets element id
raw_tweets_trimmed <- data.frame(element_id = seq(1:length(raw_tweets$Title)), 
                       Date = raw_tweets$Date, 
                       Title = raw_tweets$Title)

#get sentences from tweets 
tweet_text <- sentimentr::get_sentences(raw_tweets$Title)

tweet_sent <- sentimentr::sentiment(tweet_text)

#join with sentence data
tweet_sent <- inner_join(raw_tweets, tweet_sent, by = "element_id")
```

Use the Bing sentiment analysis lexicon.

```{r}
bing_sent <- get_sentiments('bing') #grab the bing sentiment lexicon from tidytext

#test
head(bing_sent, n = 5) %>% 
  kable()
```

```{r}

# catgorize for ggplot
tweet_sent <- tweet_sent %>%  
  mutate(sent_cat = case_when(
    sentiment < 0 ~ "negative",
    sentiment == 0 ~ "neutral",
    sentiment > 0 ~ "positive"
  ))
```

## Brandwatch

```{r}
brandwatch_sent <- data.frame(element_id = seq(1:length(raw_tweets$Title)), 
                       Date = raw_tweets$Date, 
                       Title = raw_tweets$Title,
                       Sentiment = raw_tweets$Sentiment)

sent_comp <- left_join(brandwatch_sent, tweet_sent, by = "element_id")
```

