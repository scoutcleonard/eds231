---
title: 'Topic 7: Word Embeddings'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This week's Rmd file here: <https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/topic_7.Rmd>

```{r packages, include = FALSE}
library(here)
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba) #singluar value decomposition
library(broom) # creating search_synonym function
library(textdata)
library(ggplot2)
library(dplyr)
library(knitr)
#https://semantle.com/
```

Today we are using climbing incident data from this repo: <https://github.com/ecaroom/climbing-accidents>. Some analysis (in Excel) on the data was written up into a Rock and Ice magazine article.

But I've constructed our data set (link below) by pulling a few key variables including the full text of each incident report.

```{r data,}
incidents_data_link <- "https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/825b159b6da4c7040ce8295b9eae2fbbe9991ffd/dat/climbing_report_text.csv"

incidents_df <- read_csv(incidents_data_link)
```

First, let's calculate the unigram *probabilities*, how often we see each word in this corpus.

```{r unigrams}
unigram_probs <- incidents_df %>%
    unnest_tokens(word, Text) %>%
    anti_join(stop_words, by = 'word') %>%
    count(word, sort = TRUE) %>%
    mutate(p = n / sum(n)) 

head(unigram_probs, 5) %>% 
  kable()
```

Next, we need to know *how ofte*n we find each word near each other word -- the skipgram probabilities. This is where we use the sliding window.

```{r}
skipgrams <- incidents_df %>%
    unnest_tokens(ngram, Text, token = "ngrams", n = 5) %>%
    mutate(ngramID = row_number()) %>% 
    tidyr::unite(skipgramID, ID, ngramID) %>%
    unnest_tokens(word, ngram) %>%
    anti_join(stop_words, by = 'word')

head(skipgrams, 5) %>% 
  kable()
```

```{r}
#calculate probabilities
skipgram_probs <- skipgrams %>%
    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%
    mutate(p = n / sum(n))

head(skipgram_probs, 5) %>% 
  kable()
```

Having all the skipgram windows lets us calculate how often words together occur within a window, relative to their total occurrences in the data. We do this using the *point-wise mutual information (PMI)*. It's the logarithm of the probability of finding two words together, normalized for the probability of finding each of the words alone. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

```{r norm-prob}
#normalize probabilities
normalized_prob <- skipgram_probs %>%
    filter(n > 20) %>%
    rename(word1 = item1, word2 = item2) %>%
    left_join(unigram_probs %>%
                  select(word1 = word, p1 = p),
              by = "word1") %>%
    left_join(unigram_probs %>%
                  select(word2 = word, p2 = p),
              by = "word2") %>%
    mutate(p_together = p / p1 / p2)

#Which words are most associated with "rope"?   
normalized_prob %>% 
    filter(word1 == "rope") %>%
    arrange(-p_together)

head(normalized_prob, 5) %>% 
  kable()
```

Now we convert to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
pmi_matrix <- normalized_prob %>%
    mutate(pmi = log10(p_together)) %>%
    cast_sparse(word1, word2, pmi)    
 
#remove missing data
pmi_matrix@x[is.na(pmi_matrix@x)] <- 0

#run SVD using irlba() which is good for sparse matrices
pmi_svd <- irlba(pmi_matrix, 100, maxit = 500) #Reducing to 100 dimensions

#next we output the word vectors:
word_vectors <- pmi_svd$u

rownames(word_vectors) <- rownames(pmi_matrix)
```

```{r syn-function}
search_synonyms <- function(word_vectors, selected_vector) {
  
  dat <- word_vectors %*% selected_vector
    
  similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])
  similarities %>%
    arrange(-similarity) %>%
    select(c(2,3))

}
```

```{r find-synonyms}
fall <- search_synonyms(word_vectors,word_vectors["fall",])

slip <- search_synonyms(word_vectors,word_vectors["slip",])
```

```{r plot-synonyms}
slip %>%
    mutate(selected = "slip") %>%
    bind_rows(fall %>%
                mutate(selected = "fall")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~selected, scales = "free") +
  coord_flip() +
  theme(strip.text = element_text(hjust = 0, size = 12)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(x = NULL, title = "What word vectors are most similar to slip or fall?")
         
```

```{r word-math}
snow_danger <- word_vectors["snow",] + word_vectors["danger",] 

search_synonyms(word_vectors, snow_danger)

no_snow_danger <- word_vectors["danger",] - word_vectors["snow",] 

search_synonyms(word_vectors, no_snow_danger)
```

### Assignment

Download a set of pretrained vectors, GloVe, and explore them.

Grab data here: <!-- download.file('<https://nlp.stanford.edu/data/glove.6B.zip>',destfile = 'glove.6B.zip')  --> <!-- unzip('glove.6B.zip')  --> <!-- Use this file: 'glove.6B.300d.txt' -->

1.  Recreate the analyses in the last three chunks (find-synonyms, plot-synonyms, word-math) with the GloVe embeddings. How are they different from the embeddings created from the climbing accident data? Why do you think they are different?

2.  Run the classic word math equation, "king" - "man" = ?

3.  Think of three new word math equations. They can involve any words you'd like, whatever catches your interest.
