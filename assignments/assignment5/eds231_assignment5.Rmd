---
title: "EDS 231: Assignment 1"
author: "Scout Leonard"
date: '`r format(Sys.time(), "%m/%d/%Y")`'
output: pdf_document
header-includes:
  - \setlength{\parindent}{1em}
  - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(fig.width = 6, 
                      fig.height = 5, 
                      echo = TRUE, 
                      message = FALSE, 
                      warning = FALSE)
```

# Load Libraries 

```{r load_libraries}
library(here)
library(pdftools)
library(quanteda)
library(tm)
library(topicmodels)
library(ldatuning)
library(tidyverse)
library(tidytext)
library(reshape2)
library(knitr)
```

# Set Up

## Read in data: 

```{r}
#grab data here: 
comments_df <- read_csv("https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv")
```

## Corpus: 

```{r corpus}
epa_corp <- corpus(x = comments_df, text_field = "text")

epa_corp.stats <- summary(epa_corp)

head(epa_corp.stats, n = 10) %>% 
  kable()
```

## Tokenize Corpus: 

```{r tokens}
toks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)

#I added some project-specific stop words here
add_stops <- c(stopwords("en"),"environmental", "justice", "ej", "epa", "public", "comment")

toks1 <- tokens_select(toks, pattern = add_stops, selection = "remove")
```

## Convert tokens to a document frame matrix:

```{r dfm}
#construct dfm from tokens
dfm_comm <- dfm(toks1, tolower = TRUE)

#apply a stemmer to words in dfm
dfm <- dfm_wordstem(dfm_comm)

#remove terms only appearing in one doc (min_termfreq = 10)
dfm <- dfm_trim(dfm, min_docfreq = 2) 

#remove rows (docs) with all zeros
sel_idx <- slam::row_sums(dfm) > 0 

#comments_df <- dfm[sel_idx, ]
dfm <- dfm[sel_idx, ]
```

## LDA Modelling: 

```{r LDS_modelling}
k <- 9 

#nTerms(dfm_comm) 
topicModel_k9 <- LDA(dfm,
                     k,
                     method = "Gibbs",
                     control = list(iter = 500, verbose = 25))

tmResult <- posterior(topicModel_k9)

attributes(tmResult)
#nTerms(dfm_comm)   
beta <- tmResult$terms   # get beta from results
dim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab
terms(topicModel_k9, 10)
```


# Assignment

Run three more models and select the overall best value for k (the number of topics) - include some justification for your selection: theory, FindTopicsNumber() optimization metrics, interpretability, LDAvis

## Model 1

## Model 2

## Model 3